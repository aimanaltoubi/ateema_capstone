{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83a3b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fd5aa0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV file created successfully.\n"
     ]
    }
   ],
   "source": [
    "## Romantic Resturants/candlelight\n",
    "\n",
    "# Function to scrape image URLs and count <p> tags within a specific section\n",
    "def scrape_images_and_paragraphs(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the specific section with <article> tag and class \"post py-3 pt-md-4 px-1 px-md-5 card-body\"\n",
    "        section = soup.find('article', class_='post py-3 pt-md-4 px-1 px-md-5 card-body')\n",
    "\n",
    "        # Find all <img> tags within the section\n",
    "        images = section.find_all('img')\n",
    "\n",
    "        # Create lists to store image URLs and paragraph counts\n",
    "        img_urls = []\n",
    "        p_counts = []\n",
    "\n",
    "        # Loop through each image\n",
    "        for img in images:\n",
    "            # Check if the 'src' attribute exists\n",
    "            if 'src' in img.attrs:\n",
    "                # Get the image URL\n",
    "                img_url = img['src']\n",
    "                img_urls.append(img_url)\n",
    "\n",
    "                # Count <p> tags before the image within the section\n",
    "                p_count = len(img.find_all_previous('p'))\n",
    "                p_counts.append(p_count)\n",
    "\n",
    "        return img_urls, p_counts\n",
    "    else:\n",
    "        # If the request was not successful, print an error message\n",
    "        print(\"Failed to fetch the URL:\", url)\n",
    "        return None, None\n",
    "\n",
    "# Function to scrape titles, descriptions, and paragraph indexes\n",
    "def scrape_titles_descriptions_and_indexes(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all paragraphs\n",
    "        paragraphs = soup.find_all('p')\n",
    "\n",
    "        # Find all titles (text within <strong> tags) and descriptions\n",
    "        data = []\n",
    "        for idx, p in enumerate(paragraphs):\n",
    "            strong_tags = p.find_all('strong')\n",
    "            if strong_tags:\n",
    "                title = strong_tags[0].get_text()\n",
    "                description = p.get_text().replace(title, '').strip()\n",
    "                data.append({'title': title, 'description': description, 'Paragraph Index': idx, 'category': 'romantic restaurants'})\n",
    "\n",
    "        return data\n",
    "    else:\n",
    "        # If the request was not successful, print an error message\n",
    "        print(\"Failed to fetch the URL:\", url)\n",
    "        return None\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = 'https://www.choosechicago.com/articles/food-drink/romantic-candlelight-dining/'\n",
    "\n",
    "# Scrape image URLs and paragraph counts\n",
    "img_urls, p_counts = scrape_images_and_paragraphs(url)\n",
    "\n",
    "# Scrape titles, descriptions, and paragraph indexes\n",
    "title_description_data = scrape_titles_descriptions_and_indexes(url)\n",
    "\n",
    "# Create DataFrames from scraped data\n",
    "df_images = pd.DataFrame({'image_url': img_urls, 'Paragraphs Before Image': p_counts, 'category': 'image'})\n",
    "df_titles = pd.DataFrame(title_description_data)\n",
    "\n",
    "# Merge the DataFrames based on paragraph index\n",
    "merged_df = pd.merge(df_titles, df_images, left_on='Paragraph Index', right_on='Paragraphs Before Image', how='left')\n",
    "\n",
    "# Drop unnecessary columns (category_y)\n",
    "merged_df.drop(columns=['Paragraph Index', 'Paragraphs Before Image', 'category_y'], inplace=True)\n",
    "\n",
    "# Save the combined data to a CSV file\n",
    "merged_df.to_csv('stage1_data_CoupleRomantic.csv', index=False)\n",
    "\n",
    "print(\"Combined CSV file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19d8a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Romantic Resturants/candlelight cont.\n",
    "\n",
    "\n",
    "# Read the CSV file into a DataFrame, skipping the first row\n",
    "df = pd.read_csv('stage1_data_CoupleRomantic.csv', skiprows=[1])\n",
    "# print(df.columns)\n",
    "\n",
    "punctuation = '!‚Äù$%&\\‚Äô()*+,-./:;<=>?[\\\\]^_`{|}~‚Ä¢@¬©'\n",
    "\n",
    "def remove_links(text):\n",
    "    \"\"\"Takes a string and removes web links from it\"\"\"\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove http links\n",
    "    text = re.sub(r'bit.ly/\\S+', '', text)  # remove bitly links\n",
    "    text = text.strip('[link]')  # remove [links]\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'pic.twitter\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_tags(text):\n",
    "    remove = re.compile(r'')\n",
    "    return re.sub(remove, '', text)\n",
    "\n",
    "\n",
    "def remove_users(text):\n",
    "    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n",
    "    text = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove re-tweet\n",
    "    text = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove tweeted at\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    \"\"\"Takes a string and removes any hash tags\"\"\"\n",
    "    text = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove hash tags\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_av(text):\n",
    "    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n",
    "    text = re.sub('VIDEO:', '', text)  # remove 'VIDEO:' from start of tweet\n",
    "    text = re.sub('AUDIO:', '', text)  # remove 'AUDIO:' from start of tweet\n",
    "    return text\n",
    "\n",
    "\n",
    "def basic_clean(text):\n",
    "    \"\"\"Main master function to clean tweets only without tokenization or removal of stopwords\"\"\"\n",
    "    text = remove_users(text)\n",
    "    text = remove_links(text)\n",
    "    text = remove_hashtags(text)\n",
    "    text = remove_tags(text)\n",
    "    text = remove_av(text)\n",
    "    text = text.lower()  # lower case\n",
    "    text = re.sub('[' + punctuation + ']+', ' ', text)  # strip punctuation\n",
    "    text = re.sub('\\s+', ' ', text)  # remove double spacing\n",
    "    text = re.sub('([0-9]+)', '', text)  # remove numbers\n",
    "    text = re.sub('üìù ‚Ä¶', '', text)\n",
    "    text = re.sub('‚Äì', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    return [x for x in words if x not in stop_words]\n",
    "\n",
    "\n",
    "def lemmatize_word(text):\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    return \" \".join([wordnet.lemmatize(word) for word in text])\n",
    "\n",
    "# Apply basic cleaning to the 'description' column and store the result in 'text_cleaned'\n",
    "df['text_cleaned'] = df['description'].apply(basic_clean)\n",
    "# df['text_cleaned'] = df['text_cleaned'].apply(remove_stopwords)\n",
    "# df['text_cleaned'] = df['text_cleaned'].apply(lemmatize_word)\n",
    "\n",
    "df.to_csv('stage1_data_CoupleRomantic.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b46cf792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV file created successfully.\n"
     ]
    }
   ],
   "source": [
    "## Dinner Cruises\n",
    "\n",
    "# Function to scrape image URLs and count <p> tags within a specific section\n",
    "def scrape_images_and_paragraphs(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the specific section with <article> tag and class \"post py-3 pt-md-4 px-1 px-md-5 card-body\"\n",
    "        section = soup.find('article', class_='post py-3 pt-md-4 px-1 px-md-5 card-body')\n",
    "\n",
    "        # Find all <img> tags within the section\n",
    "        images = section.find_all('img')\n",
    "\n",
    "        # Create lists to store image URLs and paragraph counts\n",
    "        img_urls = []\n",
    "        p_counts = []\n",
    "\n",
    "        # Loop through each image\n",
    "        for img in images:\n",
    "            # Check if the 'src' attribute exists\n",
    "            if 'src' in img.attrs:\n",
    "                # Get the image URL\n",
    "                img_url = img['src']\n",
    "                img_urls.append(img_url)\n",
    "\n",
    "                # Count <p> tags before the image within the section\n",
    "                p_count = len(img.find_all_previous('p'))\n",
    "                p_counts.append(p_count)\n",
    "\n",
    "        return img_urls, p_counts\n",
    "    else:\n",
    "        # If the request was not successful, print an error message\n",
    "        print(\"Failed to fetch the URL:\", url)\n",
    "        return None, None\n",
    "\n",
    "# Function to scrape titles, descriptions, and paragraph indexes\n",
    "def scrape_titles_descriptions_and_indexes(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all paragraphs\n",
    "        paragraphs = soup.find_all('p')\n",
    "\n",
    "        # Find all titles (text within <strong> tags) and descriptions\n",
    "        data = []\n",
    "        for idx, p in enumerate(paragraphs):\n",
    "            strong_tags = p.find_all('strong')\n",
    "            if strong_tags:\n",
    "                title = strong_tags[0].get_text()\n",
    "                description = p.get_text().replace(title, '').strip()\n",
    "                data.append({'title': title, 'description': description, 'Paragraph Index': idx, 'category': 'dinner cruises'})\n",
    "\n",
    "        return data\n",
    "    else:\n",
    "        # If the request was not successful, print an error message\n",
    "        print(\"Failed to fetch the URL:\", url)\n",
    "        return None\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = 'https://www.choosechicago.com/articles/food-drink/chicago-dinner-cruises/'\n",
    "\n",
    "# Scrape image URLs and paragraph counts\n",
    "img_urls, p_counts = scrape_images_and_paragraphs(url)\n",
    "\n",
    "# Scrape titles, descriptions, and paragraph indexes\n",
    "title_description_data = scrape_titles_descriptions_and_indexes(url)\n",
    "\n",
    "# Create DataFrames from scraped data\n",
    "df_images = pd.DataFrame({'image_url': img_urls, 'Paragraphs Before Image': p_counts, 'category': 'image'})\n",
    "df_titles = pd.DataFrame(title_description_data)\n",
    "\n",
    "# Merge the DataFrames based on paragraph index\n",
    "merged_df = pd.merge(df_titles, df_images, left_on='Paragraph Index', right_on='Paragraphs Before Image', how='left')\n",
    "\n",
    "# Drop unnecessary columns (category_y)\n",
    "merged_df.drop(columns=['Paragraph Index', 'Paragraphs Before Image', 'category_y'], inplace=True)\n",
    "\n",
    "# Save the combined data to a CSV file\n",
    "merged_df.to_csv('stage1_data_CoupleCruises.csv', index=False)\n",
    "\n",
    "print(\"Combined CSV file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "792c4658",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Romantic Resturants/candlelight cont.\n",
    "\n",
    "\n",
    "# Read the CSV file into a DataFrame, skipping the first row\n",
    "df = pd.read_csv('stage1_data_CoupleCruises.csv', skiprows=[1])\n",
    "# print(df.columns)\n",
    "\n",
    "punctuation = '!‚Äù$%&\\‚Äô()*+,-./:;<=>?[\\\\]^_`{|}~‚Ä¢@¬©'\n",
    "\n",
    "def remove_links(text):\n",
    "    \"\"\"Takes a string and removes web links from it\"\"\"\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove http links\n",
    "    text = re.sub(r'bit.ly/\\S+', '', text)  # remove bitly links\n",
    "    text = text.strip('[link]')  # remove [links]\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'pic.twitter\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_tags(text):\n",
    "    remove = re.compile(r'')\n",
    "    return re.sub(remove, '', text)\n",
    "\n",
    "\n",
    "def remove_users(text):\n",
    "    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n",
    "    text = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove re-tweet\n",
    "    text = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove tweeted at\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    \"\"\"Takes a string and removes any hash tags\"\"\"\n",
    "    text = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove hash tags\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_av(text):\n",
    "    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n",
    "    text = re.sub('VIDEO:', '', text)  # remove 'VIDEO:' from start of tweet\n",
    "    text = re.sub('AUDIO:', '', text)  # remove 'AUDIO:' from start of tweet\n",
    "    return text\n",
    "\n",
    "\n",
    "def basic_clean(text):\n",
    "    \"\"\"Main master function to clean tweets only without tokenization or removal of stopwords\"\"\"\n",
    "    text = remove_users(text)\n",
    "    text = remove_links(text)\n",
    "    text = remove_hashtags(text)\n",
    "    text = remove_tags(text)\n",
    "    text = remove_av(text)\n",
    "    text = text.lower()  # lower case\n",
    "    text = re.sub('[' + punctuation + ']+', ' ', text)  # strip punctuation\n",
    "    text = re.sub('\\s+', ' ', text)  # remove double spacing\n",
    "    text = re.sub('([0-9]+)', '', text)  # remove numbers\n",
    "    text = re.sub('üìù ‚Ä¶', '', text)\n",
    "    text = re.sub('‚Äì', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    return [x for x in words if x not in stop_words]\n",
    "\n",
    "\n",
    "def lemmatize_word(text):\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    return \" \".join([wordnet.lemmatize(word) for word in text])\n",
    "\n",
    "# Apply basic cleaning to the 'description' column and store the result in 'text_cleaned'\n",
    "df['text_cleaned'] = df['description'].apply(basic_clean)\n",
    "# df['text_cleaned'] = df['text_cleaned'].apply(remove_stopwords)\n",
    "# df['text_cleaned'] = df['text_cleaned'].apply(lemmatize_word)\n",
    "\n",
    "df.to_csv('stage1_data_CoupleCruises.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce672eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV file created successfully.\n"
     ]
    }
   ],
   "source": [
    "## Waterfront Restaurants \n",
    "\n",
    "# Function to scrape image URLs and count <p> tags within a specific section\n",
    "def scrape_images_and_paragraphs(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the specific section with <article> tag and class \"post py-3 pt-md-4 px-1 px-md-5 card-body\"\n",
    "        section = soup.find('article', class_='post py-3 pt-md-4 px-1 px-md-5 card-body')\n",
    "\n",
    "        # Find all <img> tags within the section\n",
    "        images = section.find_all('img')\n",
    "\n",
    "        # Create lists to store image URLs and paragraph counts\n",
    "        img_urls = []\n",
    "        p_counts = []\n",
    "\n",
    "        # Loop through each image\n",
    "        for img in images:\n",
    "            # Check if the 'src' attribute exists\n",
    "            if 'src' in img.attrs:\n",
    "                # Get the image URL\n",
    "                img_url = img['src']\n",
    "                img_urls.append(img_url)\n",
    "\n",
    "                # Count <p> tags before the image within the section\n",
    "                p_count = len(img.find_all_previous('p'))\n",
    "                p_counts.append(p_count)\n",
    "\n",
    "        return img_urls, p_counts\n",
    "    else:\n",
    "        # If the request was not successful, print an error message\n",
    "        print(\"Failed to fetch the URL:\", url)\n",
    "        return None, None\n",
    "\n",
    "# Function to scrape titles, descriptions, and paragraph indexes\n",
    "def scrape_titles_descriptions_and_indexes(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all paragraphs\n",
    "        paragraphs = soup.find_all('p')\n",
    "\n",
    "        # Find all titles (text within <strong> tags) and descriptions\n",
    "        data = []\n",
    "        for idx, p in enumerate(paragraphs):\n",
    "            strong_tags = p.find_all('strong')\n",
    "            if strong_tags:\n",
    "                title = strong_tags[0].get_text()\n",
    "                description = p.get_text().replace(title, '').strip()\n",
    "                data.append({'title': title, 'description': description, 'Paragraph Index': idx, 'category': 'waterfront restaurants'})\n",
    "\n",
    "        return data\n",
    "    else:\n",
    "        # If the request was not successful, print an error message\n",
    "        print(\"Failed to fetch the URL:\", url)\n",
    "        return None\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = 'https://www.choosechicago.com/articles/food-drink/head-to-the-beach-for-chicago-summer-eats/'\n",
    "\n",
    "# Scrape image URLs and paragraph counts\n",
    "img_urls, p_counts = scrape_images_and_paragraphs(url)\n",
    "\n",
    "# Scrape titles, descriptions, and paragraph indexes\n",
    "title_description_data = scrape_titles_descriptions_and_indexes(url)\n",
    "\n",
    "# Create DataFrames from scraped data\n",
    "df_images = pd.DataFrame({'image_url': img_urls, 'Paragraphs Before Image': p_counts, 'category': 'image'})\n",
    "df_titles = pd.DataFrame(title_description_data)\n",
    "\n",
    "# Merge the DataFrames based on paragraph index\n",
    "merged_df = pd.merge(df_titles, df_images, left_on='Paragraph Index', right_on='Paragraphs Before Image', how='left')\n",
    "\n",
    "# Drop unnecessary columns (category_y)\n",
    "merged_df.drop(columns=['Paragraph Index', 'Paragraphs Before Image', 'category_y'], inplace=True)\n",
    "\n",
    "# Save the combined data to a CSV file\n",
    "merged_df.to_csv('stage1_data_CoupleWaterfront.csv', index=False)\n",
    "\n",
    "print(\"Combined CSV file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0a4c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Waterfront Resturants cont.\n",
    "\n",
    "\n",
    "# Read the CSV file into a DataFrame, skipping the first row\n",
    "df = pd.read_csv('stage1_data_CoupleWaterfront.csv', skiprows=[1])\n",
    "# print(df.columns)\n",
    "\n",
    "punctuation = '!‚Äù$%&\\‚Äô()*+,-./:;<=>?[\\\\]^_`{|}~‚Ä¢@¬©'\n",
    "\n",
    "def remove_links(text):\n",
    "    \"\"\"Takes a string and removes web links from it\"\"\"\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove http links\n",
    "    text = re.sub(r'bit.ly/\\S+', '', text)  # remove bitly links\n",
    "    text = text.strip('[link]')  # remove [links]\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'pic.twitter\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_tags(text):\n",
    "    remove = re.compile(r'')\n",
    "    return re.sub(remove, '', text)\n",
    "\n",
    "\n",
    "def remove_users(text):\n",
    "    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n",
    "    text = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove re-tweet\n",
    "    text = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove tweeted at\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    \"\"\"Takes a string and removes any hash tags\"\"\"\n",
    "    text = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove hash tags\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_av(text):\n",
    "    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n",
    "    text = re.sub('VIDEO:', '', text)  # remove 'VIDEO:' from start of tweet\n",
    "    text = re.sub('AUDIO:', '', text)  # remove 'AUDIO:' from start of tweet\n",
    "    return text\n",
    "\n",
    "\n",
    "def basic_clean(text):\n",
    "    \"\"\"Main master function to clean tweets only without tokenization or removal of stopwords\"\"\"\n",
    "    text = remove_users(text)\n",
    "    text = remove_links(text)\n",
    "    text = remove_hashtags(text)\n",
    "    text = remove_tags(text)\n",
    "    text = remove_av(text)\n",
    "    text = text.lower()  # lower case\n",
    "    text = re.sub('[' + punctuation + ']+', ' ', text)  # strip punctuation\n",
    "    text = re.sub('\\s+', ' ', text)  # remove double spacing\n",
    "    text = re.sub('([0-9]+)', '', text)  # remove numbers\n",
    "    text = re.sub('üìù ‚Ä¶', '', text)\n",
    "    text = re.sub('‚Äì', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    return [x for x in words if x not in stop_words]\n",
    "\n",
    "\n",
    "def lemmatize_word(text):\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    return \" \".join([wordnet.lemmatize(word) for word in text])\n",
    "\n",
    "# Apply basic cleaning to the 'description' column and store the result in 'text_cleaned'\n",
    "df['text_cleaned'] = df['description'].apply(basic_clean)\n",
    "# df['text_cleaned'] = df['text_cleaned'].apply(remove_stopwords)\n",
    "# df['text_cleaned'] = df['text_cleaned'].apply(lemmatize_word)\n",
    "\n",
    "df.to_csv('stage1_data_CoupleWaterfront.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88f1f1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV file created successfully.\n"
     ]
    }
   ],
   "source": [
    "## Riverwalk Restaurants and Bars\n",
    "\n",
    "\n",
    "# Function to scrape image URLs and count <p> tags within a specific section\n",
    "def scrape_images_and_paragraphs(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the specific section with <article> tag and class \"post py-3 pt-md-4 px-1 px-md-5 card-body\"\n",
    "        section = soup.find('article', class_='post py-3 pt-md-4 px-1 px-md-5 card-body')\n",
    "\n",
    "        # Find all <img> tags within the section\n",
    "        images = section.find_all('img')\n",
    "\n",
    "        # Create lists to store image URLs and paragraph counts\n",
    "        img_urls = []\n",
    "        p_counts = []\n",
    "\n",
    "        # Loop through each image\n",
    "        for img in images:\n",
    "            # Check if the 'src' attribute exists\n",
    "            if 'src' in img.attrs:\n",
    "                # Get the image URL\n",
    "                img_url = img['src']\n",
    "                img_urls.append(img_url)\n",
    "\n",
    "                # Count <p> tags before the image within the section\n",
    "                p_count = len(img.find_all_previous('p'))\n",
    "                p_counts.append(p_count)\n",
    "\n",
    "        return img_urls, p_counts\n",
    "    else:\n",
    "        # If the request was not successful, print an error message\n",
    "        print(\"Failed to fetch the URL:\", url)\n",
    "        return None, None\n",
    "\n",
    "# Function to scrape titles, descriptions, and paragraph indexes\n",
    "def scrape_titles_descriptions_and_indexes(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all paragraphs\n",
    "        paragraphs = soup.find_all('p')\n",
    "\n",
    "        # Find all titles (text within <strong> tags) and descriptions\n",
    "        data = []\n",
    "        for idx, p in enumerate(paragraphs):\n",
    "            strong_tags = p.find_all('strong')\n",
    "            if strong_tags:\n",
    "                title = strong_tags[0].get_text()\n",
    "                description = p.get_text().replace(title, '').strip()\n",
    "                data.append({'title': title, 'description': description, 'Paragraph Index': idx, 'category': 'riverwalk restaurants'})\n",
    "\n",
    "        return data\n",
    "    else:\n",
    "        # If the request was not successful, print an error message\n",
    "        print(\"Failed to fetch the URL:\", url)\n",
    "        return None\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = 'https://www.choosechicago.com/articles/food-drink/chicago-riverwalk-restaurants-and-bars/'\n",
    "\n",
    "# Scrape image URLs and paragraph counts\n",
    "img_urls, p_counts = scrape_images_and_paragraphs(url)\n",
    "\n",
    "# Scrape titles, descriptions, and paragraph indexes\n",
    "title_description_data = scrape_titles_descriptions_and_indexes(url)\n",
    "\n",
    "# Create DataFrames from scraped data\n",
    "df_images = pd.DataFrame({'image_url': img_urls, 'Paragraphs Before Image': p_counts, 'category': 'image'})\n",
    "df_titles = pd.DataFrame(title_description_data)\n",
    "\n",
    "# Merge the DataFrames based on paragraph index\n",
    "merged_df = pd.merge(df_titles, df_images, left_on='Paragraph Index', right_on='Paragraphs Before Image', how='left')\n",
    "\n",
    "# Drop unnecessary columns (category_y)\n",
    "merged_df.drop(columns=['Paragraph Index', 'Paragraphs Before Image', 'category_y'], inplace=True)\n",
    "\n",
    "# Save the combined data to a CSV file\n",
    "merged_df.to_csv('stage1_data_CoupleRiverwalk.csv', index=False)\n",
    "\n",
    "print(\"Combined CSV file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7582467",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Riverwalk Restaurants and Bars cont.\n",
    "\n",
    "\n",
    "# Read the CSV file into a DataFrame, skipping the first row\n",
    "df = pd.read_csv('stage1_data_CoupleRiverwalk.csv', skiprows=[1])\n",
    "# print(df.columns)\n",
    "\n",
    "punctuation = '!‚Äù$%&\\‚Äô()*+,-./:;<=>?[\\\\]^_`{|}~‚Ä¢@¬©'\n",
    "\n",
    "def remove_links(text):\n",
    "    \"\"\"Takes a string and removes web links from it\"\"\"\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove http links\n",
    "    text = re.sub(r'bit.ly/\\S+', '', text)  # remove bitly links\n",
    "    text = text.strip('[link]')  # remove [links]\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'pic.twitter\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_tags(text):\n",
    "    remove = re.compile(r'')\n",
    "    return re.sub(remove, '', text)\n",
    "\n",
    "\n",
    "def remove_users(text):\n",
    "    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n",
    "    text = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove re-tweet\n",
    "    text = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove tweeted at\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    \"\"\"Takes a string and removes any hash tags\"\"\"\n",
    "    text = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove hash tags\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_av(text):\n",
    "    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n",
    "    text = re.sub('VIDEO:', '', text)  # remove 'VIDEO:' from start of tweet\n",
    "    text = re.sub('AUDIO:', '', text)  # remove 'AUDIO:' from start of tweet\n",
    "    return text\n",
    "\n",
    "\n",
    "def basic_clean(text):\n",
    "    \"\"\"Main master function to clean tweets only without tokenization or removal of stopwords\"\"\"\n",
    "    text = remove_users(text)\n",
    "    text = remove_links(text)\n",
    "    text = remove_hashtags(text)\n",
    "    text = remove_tags(text)\n",
    "    text = remove_av(text)\n",
    "    text = text.lower()  # lower case\n",
    "    text = re.sub('[' + punctuation + ']+', ' ', text)  # strip punctuation\n",
    "    text = re.sub('\\s+', ' ', text)  # remove double spacing\n",
    "    text = re.sub('([0-9]+)', '', text)  # remove numbers\n",
    "    text = re.sub('üìù ‚Ä¶', '', text)\n",
    "    text = re.sub('‚Äì', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    return [x for x in words if x not in stop_words]\n",
    "\n",
    "\n",
    "def lemmatize_word(text):\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    return \" \".join([wordnet.lemmatize(word) for word in text])\n",
    "\n",
    "# Apply basic cleaning to the 'description' column and store the result in 'text_cleaned'\n",
    "df['text_cleaned'] = df['description'].apply(basic_clean)\n",
    "# df['text_cleaned'] = df['text_cleaned'].apply(remove_stopwords)\n",
    "# df['text_cleaned'] = df['text_cleaned'].apply(lemmatize_word)\n",
    "\n",
    "df.to_csv('stage1_data_CoupleRiverwalk.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "004b8d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV file created successfully.\n"
     ]
    }
   ],
   "source": [
    "## Outdoor Dining\n",
    "\n",
    "# Function to scrape image URLs and count <p> tags within a specific section\n",
    "def scrape_images_and_paragraphs(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the specific section with <article> tag and class \"post py-3 pt-md-4 px-1 px-md-5 card-body\"\n",
    "        section = soup.find('article', class_='post py-3 pt-md-4 px-1 px-md-5 card-body')\n",
    "\n",
    "        # Find all <img> tags within the section\n",
    "        images = section.find_all('img')\n",
    "\n",
    "        # Create lists to store image URLs and paragraph counts\n",
    "        img_urls = []\n",
    "        p_counts = []\n",
    "\n",
    "        # Loop through each image\n",
    "        for img in images:\n",
    "            # Check if the 'src' attribute exists\n",
    "            if 'src' in img.attrs:\n",
    "                # Get the image URL\n",
    "                img_url = img['src']\n",
    "                img_urls.append(img_url)\n",
    "\n",
    "                # Count <p> tags before the image within the section\n",
    "                p_count = len(img.find_all_previous('p'))\n",
    "                p_counts.append(p_count)\n",
    "\n",
    "        return img_urls, p_counts\n",
    "    else:\n",
    "        # If the request was not successful, print an error message\n",
    "        print(\"Failed to fetch the URL:\", url)\n",
    "        return None, None\n",
    "\n",
    "# Function to scrape titles, descriptions, and paragraph indexes\n",
    "def scrape_titles_descriptions_and_indexes(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all paragraphs\n",
    "        paragraphs = soup.find_all('p')\n",
    "\n",
    "        # Find all titles (text within <strong> tags) and descriptions\n",
    "        data = []\n",
    "        for idx, p in enumerate(paragraphs):\n",
    "            strong_tags = p.find_all('strong')\n",
    "            if strong_tags:\n",
    "                title = strong_tags[0].get_text()\n",
    "                description = p.get_text().replace(title, '').strip()\n",
    "                data.append({'title': title, 'description': description, 'Paragraph Index': idx, 'category': 'outdoor dining'})\n",
    "\n",
    "        return data\n",
    "    else:\n",
    "        # If the request was not successful, print an error message\n",
    "        print(\"Failed to fetch the URL:\", url)\n",
    "        return None\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = 'https://www.choosechicago.com/articles/food-drink/chicago-patios-and-rooftop-bars-with-spectacular-views/'\n",
    "\n",
    "# Scrape image URLs and paragraph counts\n",
    "img_urls, p_counts = scrape_images_and_paragraphs(url)\n",
    "\n",
    "# Scrape titles, descriptions, and paragraph indexes\n",
    "title_description_data = scrape_titles_descriptions_and_indexes(url)\n",
    "\n",
    "# Create DataFrames from scraped data\n",
    "df_images = pd.DataFrame({'image_url': img_urls, 'Paragraphs Before Image': p_counts, 'category': 'image'})\n",
    "df_titles = pd.DataFrame(title_description_data)\n",
    "\n",
    "# Merge the DataFrames based on paragraph index\n",
    "merged_df = pd.merge(df_titles, df_images, left_on='Paragraph Index', right_on='Paragraphs Before Image', how='left')\n",
    "\n",
    "# Drop unnecessary columns (category_y)\n",
    "merged_df.drop(columns=['Paragraph Index', 'Paragraphs Before Image', 'category_y'], inplace=True)\n",
    "\n",
    "# Save the combined data to a CSV file\n",
    "merged_df.to_csv('stage1_data_CoupleOutdoor.csv', index=False)\n",
    "\n",
    "print(\"Combined CSV file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "360718b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Outdoor Dining cont.\n",
    "\n",
    "# Read the CSV file into a DataFrame, skipping the first row\n",
    "df = pd.read_csv('stage1_data_CoupleOutdoor.csv', skiprows=[1])\n",
    "# print(df.columns)\n",
    "\n",
    "punctuation = '!‚Äù$%&\\‚Äô()*+,-./:;<=>?[\\\\]^_`{|}~‚Ä¢@¬©'\n",
    "\n",
    "def remove_links(text):\n",
    "    \"\"\"Takes a string and removes web links from it\"\"\"\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove http links\n",
    "    text = re.sub(r'bit.ly/\\S+', '', text)  # remove bitly links\n",
    "    text = text.strip('[link]')  # remove [links]\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'pic.twitter\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_tags(text):\n",
    "    remove = re.compile(r'')\n",
    "    return re.sub(remove, '', text)\n",
    "\n",
    "\n",
    "def remove_users(text):\n",
    "    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n",
    "    text = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove re-tweet\n",
    "    text = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove tweeted at\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    \"\"\"Takes a string and removes any hash tags\"\"\"\n",
    "    text = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove hash tags\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_av(text):\n",
    "    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n",
    "    text = re.sub('VIDEO:', '', text)  # remove 'VIDEO:' from start of tweet\n",
    "    text = re.sub('AUDIO:', '', text)  # remove 'AUDIO:' from start of tweet\n",
    "    return text\n",
    "\n",
    "\n",
    "def basic_clean(text):\n",
    "    \"\"\"Main master function to clean tweets only without tokenization or removal of stopwords\"\"\"\n",
    "    text = remove_users(text)\n",
    "    text = remove_links(text)\n",
    "    text = remove_hashtags(text)\n",
    "    text = remove_tags(text)\n",
    "    text = remove_av(text)\n",
    "    text = text.lower()  # lower case\n",
    "    text = re.sub('[' + punctuation + ']+', ' ', text)  # strip punctuation\n",
    "    text = re.sub('\\s+', ' ', text)  # remove double spacing\n",
    "    text = re.sub('([0-9]+)', '', text)  # remove numbers\n",
    "    text = re.sub('üìù ‚Ä¶', '', text)\n",
    "    text = re.sub('‚Äì', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    return [x for x in words if x not in stop_words]\n",
    "\n",
    "\n",
    "def lemmatize_word(text):\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    return \" \".join([wordnet.lemmatize(word) for word in text])\n",
    "\n",
    "# Apply basic cleaning to the 'description' column and store the result in 'text_cleaned'\n",
    "df['text_cleaned'] = df['description'].apply(basic_clean)\n",
    "# df['text_cleaned'] = df['text_cleaned'].apply(remove_stopwords)\n",
    "# df['text_cleaned'] = df['text_cleaned'].apply(lemmatize_word)\n",
    "\n",
    "df.to_csv('stage1_data_CoupleOutdoor.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2baa0084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV file created successfully.\n"
     ]
    }
   ],
   "source": [
    "## Picnic Spots\n",
    "\n",
    "# Function to scrape image URLs and count <p> tags within a specific section\n",
    "def scrape_images_and_paragraphs(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the specific section with <article> tag and class \"post py-3 pt-md-4 px-1 px-md-5 card-body\"\n",
    "        section = soup.find('article', class_='post py-3 pt-md-4 px-1 px-md-5 card-body')\n",
    "\n",
    "        # Find all <img> tags within the section\n",
    "        images = section.find_all('img')\n",
    "\n",
    "        # Create lists to store image URLs and paragraph counts\n",
    "        img_urls = []\n",
    "        p_counts = []\n",
    "\n",
    "        # Loop through each image\n",
    "        for img in images:\n",
    "            # Check if the 'src' attribute exists\n",
    "            if 'src' in img.attrs:\n",
    "                # Get the image URL\n",
    "                img_url = img['src']\n",
    "                img_urls.append(img_url)\n",
    "\n",
    "                # Count <p> tags before the image within the section\n",
    "                p_count = len(img.find_all_previous('p'))\n",
    "                p_counts.append(p_count)\n",
    "\n",
    "        return img_urls, p_counts\n",
    "    else:\n",
    "        # If the request was not successful, print an error message\n",
    "        print(\"Failed to fetch the URL:\", url)\n",
    "        return None, None\n",
    "\n",
    "# Function to scrape titles, descriptions, and paragraph indexes\n",
    "def scrape_titles_descriptions_and_indexes(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all paragraphs\n",
    "        paragraphs = soup.find_all('p')\n",
    "\n",
    "        # Find all titles (text within <strong> tags) and descriptions\n",
    "        data = []\n",
    "        for idx, p in enumerate(paragraphs):\n",
    "            strong_tags = p.find_all('strong')\n",
    "            if strong_tags:\n",
    "                title = strong_tags[0].get_text()\n",
    "                description = p.get_text().replace(title, '').strip()\n",
    "                data.append({'title': title, 'description': description, 'Paragraph Index': idx, 'category': 'picnic'})\n",
    "\n",
    "        return data\n",
    "    else:\n",
    "        # If the request was not successful, print an error message\n",
    "        print(\"Failed to fetch the URL:\", url)\n",
    "        return None\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = 'https://www.choosechicago.com/blog/arts-culture-entertainment/seven-chicago-picnic-spots/'\n",
    "\n",
    "# Scrape image URLs and paragraph counts\n",
    "img_urls, p_counts = scrape_images_and_paragraphs(url)\n",
    "\n",
    "# Scrape titles, descriptions, and paragraph indexes\n",
    "title_description_data = scrape_titles_descriptions_and_indexes(url)\n",
    "\n",
    "# Create DataFrames from scraped data\n",
    "df_images = pd.DataFrame({'image_url': img_urls, 'Paragraphs Before Image': p_counts, 'category': 'image'})\n",
    "df_titles = pd.DataFrame(title_description_data)\n",
    "\n",
    "# Merge the DataFrames based on paragraph index\n",
    "merged_df = pd.merge(df_titles, df_images, left_on='Paragraph Index', right_on='Paragraphs Before Image', how='left')\n",
    "\n",
    "# Drop unnecessary columns (category_y)\n",
    "merged_df.drop(columns=['Paragraph Index', 'Paragraphs Before Image', 'category_y'], inplace=True)\n",
    "\n",
    "# Save the combined data to a CSV file\n",
    "merged_df.to_csv('stage1_data_CouplePicnic.csv', index=False)\n",
    "\n",
    "print(\"Combined CSV file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3660ef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Outdoor Dining cont.\n",
    "\n",
    "# Read the CSV file into a DataFrame, skipping the first row\n",
    "df = pd.read_csv('stage1_data_CouplePicnic.csv', skiprows=[1])\n",
    "# print(df.columns)\n",
    "\n",
    "punctuation = '!‚Äù$%&\\‚Äô()*+,-./:;<=>?[\\\\]^_`{|}~‚Ä¢@¬©'\n",
    "\n",
    "def remove_links(text):\n",
    "    \"\"\"Takes a string and removes web links from it\"\"\"\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove http links\n",
    "    text = re.sub(r'bit.ly/\\S+', '', text)  # remove bitly links\n",
    "    text = text.strip('[link]')  # remove [links]\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'pic.twitter\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_tags(text):\n",
    "    remove = re.compile(r'')\n",
    "    return re.sub(remove, '', text)\n",
    "\n",
    "\n",
    "def remove_users(text):\n",
    "    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n",
    "    text = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove re-tweet\n",
    "    text = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove tweeted at\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    \"\"\"Takes a string and removes any hash tags\"\"\"\n",
    "    text = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove hash tags\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_av(text):\n",
    "    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n",
    "    text = re.sub('VIDEO:', '', text)  # remove 'VIDEO:' from start of tweet\n",
    "    text = re.sub('AUDIO:', '', text)  # remove 'AUDIO:' from start of tweet\n",
    "    return text\n",
    "\n",
    "\n",
    "def basic_clean(text):\n",
    "    \"\"\"Main master function to clean tweets only without tokenization or removal of stopwords\"\"\"\n",
    "    text = remove_users(text)\n",
    "    text = remove_links(text)\n",
    "    text = remove_hashtags(text)\n",
    "    text = remove_tags(text)\n",
    "    text = remove_av(text)\n",
    "    text = text.lower()  # lower case\n",
    "    text = re.sub('[' + punctuation + ']+', ' ', text)  # strip punctuation\n",
    "    text = re.sub('\\s+', ' ', text)  # remove double spacing\n",
    "    text = re.sub('([0-9]+)', '', text)  # remove numbers\n",
    "    text = re.sub('üìù ‚Ä¶', '', text)\n",
    "    text = re.sub('‚Äì', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    return [x for x in words if x not in stop_words]\n",
    "\n",
    "\n",
    "def lemmatize_word(text):\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    return \" \".join([wordnet.lemmatize(word) for word in text])\n",
    "\n",
    "# Apply basic cleaning to the 'description' column and store the result in 'text_cleaned'\n",
    "df['text_cleaned'] = df['description'].apply(basic_clean)\n",
    "# df['text_cleaned'] = df['text_cleaned'].apply(remove_stopwords)\n",
    "# df['text_cleaned'] = df['text_cleaned'].apply(lemmatize_word)\n",
    "\n",
    "df.to_csv('stage1_data_CouplePicnic.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f1ec31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV file created successfully.\n"
     ]
    }
   ],
   "source": [
    "## Indoor Gardens\n",
    "\n",
    "# Function to scrape image URLs and count <p> tags within a specific section\n",
    "def scrape_images_and_paragraphs(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the specific section with <article> tag and class \"post py-3 pt-md-4 px-1 px-md-5 card-body\"\n",
    "        section = soup.find('article', class_='post py-3 pt-md-4 px-1 px-md-5 card-body')\n",
    "\n",
    "        # Find all <img> tags within the section\n",
    "        images = section.find_all('img')\n",
    "\n",
    "        # Create lists to store image URLs and paragraph counts\n",
    "        img_urls = []\n",
    "        p_counts = []\n",
    "\n",
    "        # Loop through each image\n",
    "        for img in images:\n",
    "            # Check if the 'src' attribute exists\n",
    "            if 'src' in img.attrs:\n",
    "                # Get the image URL\n",
    "                img_url = img['src']\n",
    "                img_urls.append(img_url)\n",
    "\n",
    "                # Count <p> tags before the image within the section\n",
    "                p_count = len(img.find_all_previous('p'))\n",
    "                p_counts.append(p_count)\n",
    "\n",
    "        return img_urls, p_counts\n",
    "    else:\n",
    "        # If the request was not successful, print an error message\n",
    "        print(\"Failed to fetch the URL:\", url)\n",
    "        return None, None\n",
    "\n",
    "# Function to scrape titles, descriptions, and paragraph indexes\n",
    "def scrape_titles_descriptions_and_indexes(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all paragraphs\n",
    "        paragraphs = soup.find_all('p')\n",
    "\n",
    "        # Find all titles (text within <strong> tags) and descriptions\n",
    "        data = []\n",
    "        for idx, p in enumerate(paragraphs):\n",
    "            strong_tags = p.find_all('strong')\n",
    "            if strong_tags:\n",
    "                title = strong_tags[0].get_text()\n",
    "                description = p.get_text().replace(title, '').strip()\n",
    "                data.append({'title': title, 'description': description, 'Paragraph Index': idx, 'category': 'indoor gardens'})\n",
    "\n",
    "        return data\n",
    "    else:\n",
    "        # If the request was not successful, print an error message\n",
    "        print(\"Failed to fetch the URL:\", url)\n",
    "        return None\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = 'https://www.choosechicago.com/articles/parks-outdoors/chicago-conservatories-and-winter-gardens/'\n",
    "\n",
    "# Scrape image URLs and paragraph counts\n",
    "img_urls, p_counts = scrape_images_and_paragraphs(url)\n",
    "\n",
    "# Scrape titles, descriptions, and paragraph indexes\n",
    "title_description_data = scrape_titles_descriptions_and_indexes(url)\n",
    "\n",
    "# Create DataFrames from scraped data\n",
    "df_images = pd.DataFrame({'image_url': img_urls, 'Paragraphs Before Image': p_counts, 'category': 'image'})\n",
    "df_titles = pd.DataFrame(title_description_data)\n",
    "\n",
    "# Merge the DataFrames based on paragraph index\n",
    "merged_df = pd.merge(df_titles, df_images, left_on='Paragraph Index', right_on='Paragraphs Before Image', how='left')\n",
    "\n",
    "# Drop unnecessary columns (category_y)\n",
    "merged_df.drop(columns=['Paragraph Index', 'Paragraphs Before Image', 'category_y'], inplace=True)\n",
    "\n",
    "# Save the combined data to a CSV file\n",
    "merged_df.to_csv('stage1_data_CoupleIndoorGardens.csv', index=False)\n",
    "\n",
    "print(\"Combined CSV file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8e66af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Indoor Gardens cont. \n",
    "\n",
    "# Read the CSV file into a DataFrame, skipping the first row\n",
    "df = pd.read_csv('stage1_data_CoupleIndoorGardens.csv', skiprows=[1])\n",
    "# print(df.columns)\n",
    "\n",
    "punctuation = '!‚Äù$%&\\‚Äô()*+,-./:;<=>?[\\\\]^_`{|}~‚Ä¢@¬©'\n",
    "\n",
    "def remove_links(text):\n",
    "    \"\"\"Takes a string and removes web links from it\"\"\"\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove http links\n",
    "    text = re.sub(r'bit.ly/\\S+', '', text)  # remove bitly links\n",
    "    text = text.strip('[link]')  # remove [links]\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'pic.twitter\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_tags(text):\n",
    "    remove = re.compile(r'')\n",
    "    return re.sub(remove, '', text)\n",
    "\n",
    "\n",
    "def remove_users(text):\n",
    "    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n",
    "    text = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove re-tweet\n",
    "    text = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove tweeted at\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    \"\"\"Takes a string and removes any hash tags\"\"\"\n",
    "    text = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove hash tags\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_av(text):\n",
    "    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n",
    "    text = re.sub('VIDEO:', '', text)  # remove 'VIDEO:' from start of tweet\n",
    "    text = re.sub('AUDIO:', '', text)  # remove 'AUDIO:' from start of tweet\n",
    "    return text\n",
    "\n",
    "\n",
    "def basic_clean(text):\n",
    "    \"\"\"Main master function to clean tweets only without tokenization or removal of stopwords\"\"\"\n",
    "    text = remove_users(text)\n",
    "    text = remove_links(text)\n",
    "    text = remove_hashtags(text)\n",
    "    text = remove_tags(text)\n",
    "    text = remove_av(text)\n",
    "    text = text.lower()  # lower case\n",
    "    text = re.sub('[' + punctuation + ']+', ' ', text)  # strip punctuation\n",
    "    text = re.sub('\\s+', ' ', text)  # remove double spacing\n",
    "    text = re.sub('([0-9]+)', '', text)  # remove numbers\n",
    "    text = re.sub('üìù ‚Ä¶', '', text)\n",
    "    text = re.sub('‚Äì', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    return [x for x in words if x not in stop_words]\n",
    "\n",
    "\n",
    "def lemmatize_word(text):\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    return \" \".join([wordnet.lemmatize(word) for word in text])\n",
    "\n",
    "# Apply basic cleaning to the 'description' column and store the result in 'text_cleaned'\n",
    "df['text_cleaned'] = df['description'].apply(basic_clean)\n",
    "# df['text_cleaned'] = df['text_cleaned'].apply(remove_stopwords)\n",
    "# df['text_cleaned'] = df['text_cleaned'].apply(lemmatize_word)\n",
    "\n",
    "df.to_csv('stage1_data_CoupleIndoorGardens.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0128bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV file created successfully.\n"
     ]
    }
   ],
   "source": [
    "## Waterfronts \n",
    "\n",
    "# Function to scrape image URLs and count <p> tags within a specific section\n",
    "def scrape_images_and_paragraphs(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the specific section with <article> tag and class \"post py-3 pt-md-4 px-1 px-md-5 card-body\"\n",
    "        section = soup.find('article', class_='post py-3 pt-md-4 px-1 px-md-5 card-body')\n",
    "\n",
    "        # Find all <img> tags within the section\n",
    "        images = section.find_all('img')\n",
    "\n",
    "        # Create lists to store image URLs and paragraph counts\n",
    "        img_urls = []\n",
    "        p_counts = []\n",
    "\n",
    "        # Loop through each image\n",
    "        for img in images:\n",
    "            # Check if the 'src' attribute exists\n",
    "            if 'src' in img.attrs:\n",
    "                # Get the image URL\n",
    "                img_url = img['src']\n",
    "                img_urls.append(img_url)\n",
    "\n",
    "                # Count <p> tags before the image within the section\n",
    "                p_count = len(img.find_all_previous('p'))\n",
    "                p_counts.append(p_count)\n",
    "\n",
    "        return img_urls, p_counts\n",
    "    else:\n",
    "        # If the request was not successful, print an error message\n",
    "        print(\"Failed to fetch the URL:\", url)\n",
    "        return None, None\n",
    "\n",
    "# Function to scrape titles, descriptions, and paragraph indexes\n",
    "def scrape_titles_descriptions_and_indexes(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all paragraphs\n",
    "        paragraphs = soup.find_all('p')\n",
    "\n",
    "        # Find all titles (text within <strong> tags) and descriptions\n",
    "        data = []\n",
    "        for idx, p in enumerate(paragraphs):\n",
    "            strong_tags = p.find_all('strong')\n",
    "            if strong_tags:\n",
    "                title = strong_tags[0].get_text()\n",
    "                description = p.get_text().replace(title, '').strip()\n",
    "                data.append({'title': title, 'description': description, 'Paragraph Index': idx, 'category': 'waterfronts'})\n",
    "\n",
    "        return data\n",
    "    else:\n",
    "        # If the request was not successful, print an error message\n",
    "        print(\"Failed to fetch the URL:\", url)\n",
    "        return None\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = 'https://www.choosechicago.com/articles/parks-outdoors/2-days-2-chicago-waterfronts/'\n",
    "\n",
    "# Scrape image URLs and paragraph counts\n",
    "img_urls, p_counts = scrape_images_and_paragraphs(url)\n",
    "\n",
    "# Scrape titles, descriptions, and paragraph indexes\n",
    "title_description_data = scrape_titles_descriptions_and_indexes(url)\n",
    "\n",
    "# Create DataFrames from scraped data\n",
    "df_images = pd.DataFrame({'image_url': img_urls, 'Paragraphs Before Image': p_counts, 'category': 'image'})\n",
    "df_titles = pd.DataFrame(title_description_data)\n",
    "\n",
    "# Merge the DataFrames based on paragraph index\n",
    "merged_df = pd.merge(df_titles, df_images, left_on='Paragraph Index', right_on='Paragraphs Before Image', how='left')\n",
    "\n",
    "# Drop unnecessary columns (category_y)\n",
    "merged_df.drop(columns=['Paragraph Index', 'Paragraphs Before Image', 'category_y'], inplace=True)\n",
    "\n",
    "# Save the combined data to a CSV file\n",
    "merged_df.to_csv('stage1_data_CoupleWaterfronts.csv', index=False)\n",
    "\n",
    "print(\"Combined CSV file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0484d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Waterfronts cont. \n",
    "\n",
    "# Read the CSV file into a DataFrame, skipping the first row\n",
    "df = pd.read_csv('stage1_data_CoupleWaterfronts.csv', skiprows=[1])\n",
    "# print(df.columns)\n",
    "\n",
    "punctuation = '!‚Äù$%&\\‚Äô()*+,-./:;<=>?[\\\\]^_`{|}~‚Ä¢@¬©'\n",
    "\n",
    "def remove_links(text):\n",
    "    \"\"\"Takes a string and removes web links from it\"\"\"\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove http links\n",
    "    text = re.sub(r'bit.ly/\\S+', '', text)  # remove bitly links\n",
    "    text = text.strip('[link]')  # remove [links]\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'pic.twitter\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_tags(text):\n",
    "    remove = re.compile(r'')\n",
    "    return re.sub(remove, '', text)\n",
    "\n",
    "\n",
    "def remove_users(text):\n",
    "    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n",
    "    text = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove re-tweet\n",
    "    text = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove tweeted at\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    \"\"\"Takes a string and removes any hash tags\"\"\"\n",
    "    text = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove hash tags\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_av(text):\n",
    "    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n",
    "    text = re.sub('VIDEO:', '', text)  # remove 'VIDEO:' from start of tweet\n",
    "    text = re.sub('AUDIO:', '', text)  # remove 'AUDIO:' from start of tweet\n",
    "    return text\n",
    "\n",
    "\n",
    "def basic_clean(text):\n",
    "    \"\"\"Main master function to clean tweets only without tokenization or removal of stopwords\"\"\"\n",
    "    text = remove_users(text)\n",
    "    text = remove_links(text)\n",
    "    text = remove_hashtags(text)\n",
    "    text = remove_tags(text)\n",
    "    text = remove_av(text)\n",
    "    text = text.lower()  # lower case\n",
    "    text = re.sub('[' + punctuation + ']+', ' ', text)  # strip punctuation\n",
    "    text = re.sub('\\s+', ' ', text)  # remove double spacing\n",
    "    text = re.sub('([0-9]+)', '', text)  # remove numbers\n",
    "    text = re.sub('üìù ‚Ä¶', '', text)\n",
    "    text = re.sub('‚Äì', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    return [x for x in words if x not in stop_words]\n",
    "\n",
    "\n",
    "def lemmatize_word(text):\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    return \" \".join([wordnet.lemmatize(word) for word in text])\n",
    "\n",
    "# Apply basic cleaning to the 'description' column and store the result in 'text_cleaned'\n",
    "df['text_cleaned'] = df['description'].apply(basic_clean)\n",
    "# df['text_cleaned'] = df['text_cleaned'].apply(remove_stopwords)\n",
    "# df['text_cleaned'] = df['text_cleaned'].apply(lemmatize_word)\n",
    "\n",
    "df.to_csv('stage1_data_CoupleWaterfronts.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9ee761c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV file created successfully.\n"
     ]
    }
   ],
   "source": [
    "## Beaches\n",
    "\n",
    "# Function to scrape image URLs and count <p> tags within a specific section\n",
    "def scrape_images_and_paragraphs(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the specific section with <article> tag and class \"post py-3 pt-md-4 px-1 px-md-5 card-body\"\n",
    "        section = soup.find('article', class_='post py-3 pt-md-4 px-1 px-md-5 card-body')\n",
    "\n",
    "        # Find all <img> tags within the section\n",
    "        images = section.find_all('img')\n",
    "\n",
    "        # Create lists to store image URLs and paragraph counts\n",
    "        img_urls = []\n",
    "        p_counts = []\n",
    "\n",
    "        # Loop through each image\n",
    "        for img in images:\n",
    "            # Check if the 'src' attribute exists\n",
    "            if 'src' in img.attrs:\n",
    "                # Get the image URL\n",
    "                img_url = img['src']\n",
    "                img_urls.append(img_url)\n",
    "\n",
    "                # Count <p> tags before the image within the section\n",
    "                p_count = len(img.find_all_previous('p'))\n",
    "                p_counts.append(p_count)\n",
    "\n",
    "        return img_urls, p_counts\n",
    "    else:\n",
    "        # If the request was not successful, print an error message\n",
    "        print(\"Failed to fetch the URL:\", url)\n",
    "        return None, None\n",
    "\n",
    "# Function to scrape titles, descriptions, and paragraph indexes\n",
    "def scrape_titles_descriptions_and_indexes(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all paragraphs\n",
    "        paragraphs = soup.find_all('p')\n",
    "\n",
    "        # Find all titles (text within <strong> tags) and descriptions\n",
    "        data = []\n",
    "        for idx, p in enumerate(paragraphs):\n",
    "            strong_tags = p.find_all('strong')\n",
    "            if strong_tags:\n",
    "                title = strong_tags[0].get_text()\n",
    "                description = p.get_text().replace(title, '').strip()\n",
    "                data.append({'title': title, 'description': description, 'Paragraph Index': idx, 'category': 'beaches'})\n",
    "\n",
    "        return data\n",
    "    else:\n",
    "        # If the request was not successful, print an error message\n",
    "        print(\"Failed to fetch the URL:\", url)\n",
    "        return None\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = 'https://www.choosechicago.com/articles/parks-outdoors/fun-in-the-sun-on-chicagos-beaches/'\n",
    "\n",
    "# Scrape image URLs and paragraph counts\n",
    "img_urls, p_counts = scrape_images_and_paragraphs(url)\n",
    "\n",
    "# Scrape titles, descriptions, and paragraph indexes\n",
    "title_description_data = scrape_titles_descriptions_and_indexes(url)\n",
    "\n",
    "# Create DataFrames from scraped data\n",
    "df_images = pd.DataFrame({'image_url': img_urls, 'Paragraphs Before Image': p_counts, 'category': 'image'})\n",
    "df_titles = pd.DataFrame(title_description_data)\n",
    "\n",
    "# Merge the DataFrames based on paragraph index\n",
    "merged_df = pd.merge(df_titles, df_images, left_on='Paragraph Index', right_on='Paragraphs Before Image', how='left')\n",
    "\n",
    "# Drop unnecessary columns (category_y)\n",
    "merged_df.drop(columns=['Paragraph Index', 'Paragraphs Before Image', 'category_y'], inplace=True)\n",
    "\n",
    "# Save the combined data to a CSV file\n",
    "merged_df.to_csv('stage1_data_CoupleBeaches.csv', index=False)\n",
    "\n",
    "print(\"Combined CSV file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ddc6964",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Beaches cont.\n",
    "\n",
    "# Read the CSV file into a DataFrame, skipping the first row\n",
    "df = pd.read_csv('stage1_data_CoupleBeaches.csv', skiprows=[1])\n",
    "# print(df.columns)\n",
    "\n",
    "punctuation = '!‚Äù$%&\\‚Äô()*+,-./:;<=>?[\\\\]^_`{|}~‚Ä¢@¬©'\n",
    "\n",
    "def remove_links(text):\n",
    "    \"\"\"Takes a string and removes web links from it\"\"\"\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove http links\n",
    "    text = re.sub(r'bit.ly/\\S+', '', text)  # remove bitly links\n",
    "    text = text.strip('[link]')  # remove [links]\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'pic.twitter\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_tags(text):\n",
    "    remove = re.compile(r'')\n",
    "    return re.sub(remove, '', text)\n",
    "\n",
    "\n",
    "def remove_users(text):\n",
    "    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n",
    "    text = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove re-tweet\n",
    "    text = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove tweeted at\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    \"\"\"Takes a string and removes any hash tags\"\"\"\n",
    "    text = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove hash tags\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_av(text):\n",
    "    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n",
    "    text = re.sub('VIDEO:', '', text)  # remove 'VIDEO:' from start of tweet\n",
    "    text = re.sub('AUDIO:', '', text)  # remove 'AUDIO:' from start of tweet\n",
    "    return text\n",
    "\n",
    "\n",
    "def basic_clean(text):\n",
    "    \"\"\"Main master function to clean tweets only without tokenization or removal of stopwords\"\"\"\n",
    "    text = remove_users(text)\n",
    "    text = remove_links(text)\n",
    "    text = remove_hashtags(text)\n",
    "    text = remove_tags(text)\n",
    "    text = remove_av(text)\n",
    "    text = text.lower()  # lower case\n",
    "    text = re.sub('[' + punctuation + ']+', ' ', text)  # strip punctuation\n",
    "    text = re.sub('\\s+', ' ', text)  # remove double spacing\n",
    "    text = re.sub('([0-9]+)', '', text)  # remove numbers\n",
    "    text = re.sub('üìù ‚Ä¶', '', text)\n",
    "    text = re.sub('‚Äì', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    return [x for x in words if x not in stop_words]\n",
    "\n",
    "\n",
    "def lemmatize_word(text):\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    return \" \".join([wordnet.lemmatize(word) for word in text])\n",
    "\n",
    "# Apply basic cleaning to the 'description' column and store the result in 'text_cleaned'\n",
    "df['text_cleaned'] = df['description'].apply(basic_clean)\n",
    "# df['text_cleaned'] = df['text_cleaned'].apply(remove_stopwords)\n",
    "# df['text_cleaned'] = df['text_cleaned'].apply(lemmatize_word)\n",
    "\n",
    "df.to_csv('stage1_data_CoupleBeaches.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "654902f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV file created successfully.\n"
     ]
    }
   ],
   "source": [
    "## Millennium Park\n",
    "\n",
    "# Function to scrape image URLs and count <p> tags within a specific section\n",
    "def scrape_images_and_paragraphs(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the specific section with <article> tag and class \"post py-3 pt-md-4 px-1 px-md-5 card-body\"\n",
    "        section = soup.find('article', class_='post py-3 pt-md-4 px-1 px-md-5 card-body')\n",
    "\n",
    "        # Find all <img> tags within the section\n",
    "        images = section.find_all('img')\n",
    "\n",
    "        # Create lists to store image URLs and paragraph counts\n",
    "        img_urls = []\n",
    "        p_counts = []\n",
    "\n",
    "        # Loop through each image\n",
    "        for img in images:\n",
    "            # Check if the 'src' attribute exists\n",
    "            if 'src' in img.attrs:\n",
    "                # Get the image URL\n",
    "                img_url = img['src']\n",
    "                img_urls.append(img_url)\n",
    "\n",
    "                # Count <p> tags before the image within the section\n",
    "                p_count = len(img.find_all_previous('p'))\n",
    "                p_counts.append(p_count)\n",
    "\n",
    "        return img_urls, p_counts\n",
    "    else:\n",
    "        # If the request was not successful, print an error message\n",
    "        print(\"Failed to fetch the URL:\", url)\n",
    "        return None, None\n",
    "\n",
    "# Function to scrape titles, descriptions, and paragraph indexes\n",
    "def scrape_titles_descriptions_and_indexes(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all paragraphs\n",
    "        paragraphs = soup.find_all('p')\n",
    "\n",
    "        # Find all titles (text within <strong> tags) and descriptions\n",
    "        data = []\n",
    "        for idx, p in enumerate(paragraphs):\n",
    "            strong_tags = p.find_all('strong')\n",
    "            if strong_tags:\n",
    "                title = strong_tags[0].get_text()\n",
    "                description = p.get_text().replace(title, '').strip()\n",
    "                data.append({'title': title, 'description': description, 'Paragraph Index': idx, 'category': 'MillenniumPark'})\n",
    "\n",
    "        return data\n",
    "    else:\n",
    "        # If the request was not successful, print an error message\n",
    "        print(\"Failed to fetch the URL:\", url)\n",
    "        return None\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = 'https://www.choosechicago.com/articles/parks-outdoors/millennium-park-campus/'\n",
    "\n",
    "# Scrape image URLs and paragraph counts\n",
    "img_urls, p_counts = scrape_images_and_paragraphs(url)\n",
    "\n",
    "# Scrape titles, descriptions, and paragraph indexes\n",
    "title_description_data = scrape_titles_descriptions_and_indexes(url)\n",
    "\n",
    "# Create DataFrames from scraped data\n",
    "df_images = pd.DataFrame({'image_url': img_urls, 'Paragraphs Before Image': p_counts, 'category': 'image'})\n",
    "df_titles = pd.DataFrame(title_description_data)\n",
    "\n",
    "# Merge the DataFrames based on paragraph index\n",
    "merged_df = pd.merge(df_titles, df_images, left_on='Paragraph Index', right_on='Paragraphs Before Image', how='left')\n",
    "\n",
    "# Drop unnecessary columns (category_y)\n",
    "merged_df.drop(columns=['Paragraph Index', 'Paragraphs Before Image', 'category_y'], inplace=True)\n",
    "\n",
    "# Save the combined data to a CSV file\n",
    "merged_df.to_csv('stage1_data_CoupleMillenniumPark.csv', index=False)\n",
    "\n",
    "print(\"Combined CSV file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05c854c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Millennium Park cont.\n",
    "\n",
    "# Read the CSV file into a DataFrame, skipping the first row\n",
    "df = pd.read_csv('stage1_data_CoupleMillenniumPark.csv', skiprows=[1])\n",
    "# print(df.columns)\n",
    "\n",
    "punctuation = '!‚Äù$%&\\‚Äô()*+,-./:;<=>?[\\\\]^_`{|}~‚Ä¢@¬©'\n",
    "\n",
    "def remove_links(text):\n",
    "    \"\"\"Takes a string and removes web links from it\"\"\"\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove http links\n",
    "    text = re.sub(r'bit.ly/\\S+', '', text)  # remove bitly links\n",
    "    text = text.strip('[link]')  # remove [links]\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'pic.twitter\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_tags(text):\n",
    "    remove = re.compile(r'')\n",
    "    return re.sub(remove, '', text)\n",
    "\n",
    "\n",
    "def remove_users(text):\n",
    "    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n",
    "    text = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove re-tweet\n",
    "    text = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove tweeted at\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    \"\"\"Takes a string and removes any hash tags\"\"\"\n",
    "    text = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove hash tags\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_av(text):\n",
    "    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n",
    "    text = re.sub('VIDEO:', '', text)  # remove 'VIDEO:' from start of tweet\n",
    "    text = re.sub('AUDIO:', '', text)  # remove 'AUDIO:' from start of tweet\n",
    "    return text\n",
    "\n",
    "\n",
    "def basic_clean(text):\n",
    "    \"\"\"Main master function to clean tweets only without tokenization or removal of stopwords\"\"\"\n",
    "    text = remove_users(text)\n",
    "    text = remove_links(text)\n",
    "    text = remove_hashtags(text)\n",
    "    text = remove_tags(text)\n",
    "    text = remove_av(text)\n",
    "    text = text.lower()  # lower case\n",
    "    text = re.sub('[' + punctuation + ']+', ' ', text)  # strip punctuation\n",
    "    text = re.sub('\\s+', ' ', text)  # remove double spacing\n",
    "    text = re.sub('([0-9]+)', '', text)  # remove numbers\n",
    "    text = re.sub('üìù ‚Ä¶', '', text)\n",
    "    text = re.sub('‚Äì', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    return [x for x in words if x not in stop_words]\n",
    "\n",
    "\n",
    "def lemmatize_word(text):\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    return \" \".join([wordnet.lemmatize(word) for word in text])\n",
    "\n",
    "# Apply basic cleaning to the 'description' column and store the result in 'text_cleaned'\n",
    "df['text_cleaned'] = df['description'].apply(basic_clean)\n",
    "# df['text_cleaned'] = df['text_cleaned'].apply(remove_stopwords)\n",
    "# df['text_cleaned'] = df['text_cleaned'].apply(lemmatize_word)\n",
    "\n",
    "df.to_csv('stage1_data_CoupleMillenniumPark.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f21d5cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV file created successfully.\n"
     ]
    }
   ],
   "source": [
    "## Date Night Ideas\n",
    "\n",
    "\n",
    "# Function to scrape image URLs and count <p> tags within a specific section\n",
    "def scrape_images_and_paragraphs(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the specific section with <article> tag and class \"post py-3 pt-md-4 px-1 px-md-5 card-body\"\n",
    "        section = soup.find('article', class_='post py-3 pt-md-4 px-1 px-md-5 card-body')\n",
    "\n",
    "        # Find all <img> tags within the section\n",
    "        images = section.find_all('img')\n",
    "\n",
    "        # Create lists to store image URLs and paragraph counts\n",
    "        img_urls = []\n",
    "        p_counts = []\n",
    "\n",
    "        # Loop through each image\n",
    "        for img in images:\n",
    "            # Check if the 'src' attribute exists\n",
    "            if 'src' in img.attrs:\n",
    "                # Get the image URL\n",
    "                img_url = img['src']\n",
    "                img_urls.append(img_url)\n",
    "\n",
    "                # Count <p> tags before the image within the section\n",
    "                p_count = len(img.find_all_previous('p'))\n",
    "                p_counts.append(p_count)\n",
    "\n",
    "        return img_urls, p_counts\n",
    "    else:\n",
    "        # If the request was not successful, print an error message\n",
    "        print(\"Failed to fetch the URL:\", url)\n",
    "        return None, None\n",
    "\n",
    "# Function to scrape titles, descriptions, and paragraph indexes\n",
    "def scrape_titles_descriptions_and_indexes(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all paragraphs\n",
    "        paragraphs = soup.find_all('p')\n",
    "\n",
    "        # Find all titles (text within <strong> tags) and descriptions\n",
    "        data = []\n",
    "        for idx, p in enumerate(paragraphs):\n",
    "            strong_tags = p.find_all('strong')\n",
    "            if strong_tags:\n",
    "                title = strong_tags[0].get_text()\n",
    "                description = p.get_text().replace(title, '').strip()\n",
    "                data.append({'title': title, 'description': description, 'Paragraph Index': idx, 'category': 'date night ideas'})\n",
    "\n",
    "        return data\n",
    "    else:\n",
    "        # If the request was not successful, print an error message\n",
    "        print(\"Failed to fetch the URL:\", url)\n",
    "        return None\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = 'https://www.choosechicago.com/articles/itineraries/3-great-chicago-date-ideas/'\n",
    "\n",
    "# Scrape image URLs and paragraph counts\n",
    "img_urls, p_counts = scrape_images_and_paragraphs(url)\n",
    "\n",
    "# Scrape titles, descriptions, and paragraph indexes\n",
    "title_description_data = scrape_titles_descriptions_and_indexes(url)\n",
    "\n",
    "# Create DataFrames from scraped data\n",
    "df_images = pd.DataFrame({'image_url': img_urls, 'Paragraphs Before Image': p_counts, 'category': 'image'})\n",
    "df_titles = pd.DataFrame(title_description_data)\n",
    "\n",
    "# Merge the DataFrames based on paragraph index\n",
    "merged_df = pd.merge(df_titles, df_images, left_on='Paragraph Index', right_on='Paragraphs Before Image', how='left')\n",
    "\n",
    "# Drop unnecessary columns (category_y)\n",
    "merged_df.drop(columns=['Paragraph Index', 'Paragraphs Before Image', 'category_y'], inplace=True)\n",
    "\n",
    "# Save the combined data to a CSV file\n",
    "merged_df.to_csv('stage1_data_CoupleDateNightIdeas.csv', index=False)\n",
    "\n",
    "print(\"Combined CSV file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de9b352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Date Night Ideas cont.\n",
    "\n",
    "\n",
    "# Read the CSV file into a DataFrame, skipping the first row\n",
    "df = pd.read_csv('stage1_data_CoupleDateNightIdeas.csv', skiprows=[1])\n",
    "# print(df.columns)\n",
    "\n",
    "punctuation = '!‚Äù$%&\\‚Äô()*+,-./:;<=>?[\\\\]^_`{|}~‚Ä¢@¬©'\n",
    "\n",
    "def remove_links(text):\n",
    "    \"\"\"Takes a string and removes web links from it\"\"\"\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove http links\n",
    "    text = re.sub(r'bit.ly/\\S+', '', text)  # remove bitly links\n",
    "    text = text.strip('[link]')  # remove [links]\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'pic.twitter\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_tags(text):\n",
    "    remove = re.compile(r'')\n",
    "    return re.sub(remove, '', text)\n",
    "\n",
    "\n",
    "def remove_users(text):\n",
    "    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n",
    "    text = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove re-tweet\n",
    "    text = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove tweeted at\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    \"\"\"Takes a string and removes any hash tags\"\"\"\n",
    "    text = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove hash tags\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_av(text):\n",
    "    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n",
    "    text = re.sub('VIDEO:', '', text)  # remove 'VIDEO:' from start of tweet\n",
    "    text = re.sub('AUDIO:', '', text)  # remove 'AUDIO:' from start of tweet\n",
    "    return text\n",
    "\n",
    "\n",
    "def basic_clean(text):\n",
    "    \"\"\"Main master function to clean tweets only without tokenization or removal of stopwords\"\"\"\n",
    "    text = remove_users(text)\n",
    "    text = remove_links(text)\n",
    "    text = remove_hashtags(text)\n",
    "    text = remove_tags(text)\n",
    "    text = remove_av(text)\n",
    "    text = text.lower()  # lower case\n",
    "    text = re.sub('[' + punctuation + ']+', ' ', text)  # strip punctuation\n",
    "    text = re.sub('\\s+', ' ', text)  # remove double spacing\n",
    "    text = re.sub('([0-9]+)', '', text)  # remove numbers\n",
    "    text = re.sub('üìù ‚Ä¶', '', text)\n",
    "    text = re.sub('‚Äì', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    return [x for x in words if x not in stop_words]\n",
    "\n",
    "\n",
    "def lemmatize_word(text):\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    return \" \".join([wordnet.lemmatize(word) for word in text])\n",
    "\n",
    "# Apply basic cleaning to the 'description' column and store the result in 'text_cleaned'\n",
    "df['text_cleaned'] = df['description'].apply(basic_clean)\n",
    "# df['text_cleaned'] = df['text_cleaned'].apply(remove_stopwords)\n",
    "# df['text_cleaned'] = df['text_cleaned'].apply(lemmatize_word)\n",
    "\n",
    "df.to_csv('stage1_data_CoupleDateNightIdeas.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9cd5bba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV file created successfully.\n"
     ]
    }
   ],
   "source": [
    "## Romantic Weekend\n",
    "\n",
    "\n",
    "# Function to scrape image URLs and count <p> tags within a specific section\n",
    "def scrape_images_and_paragraphs(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the specific section with <article> tag and class \"post py-3 pt-md-4 px-1 px-md-5 card-body\"\n",
    "        section = soup.find('article', class_='post py-3 pt-md-4 px-1 px-md-5 card-body')\n",
    "\n",
    "        # Find all <img> tags within the section\n",
    "        images = section.find_all('img')\n",
    "\n",
    "        # Create lists to store image URLs and paragraph counts\n",
    "        img_urls = []\n",
    "        p_counts = []\n",
    "\n",
    "        # Loop through each image\n",
    "        for img in images:\n",
    "            # Check if the 'src' attribute exists\n",
    "            if 'src' in img.attrs:\n",
    "                # Get the image URL\n",
    "                img_url = img['src']\n",
    "                img_urls.append(img_url)\n",
    "\n",
    "                # Count <p> tags before the image within the section\n",
    "                p_count = len(img.find_all_previous('p'))\n",
    "                p_counts.append(p_count)\n",
    "\n",
    "        return img_urls, p_counts\n",
    "    else:\n",
    "        # If the request was not successful, print an error message\n",
    "        print(\"Failed to fetch the URL:\", url)\n",
    "        return None, None\n",
    "\n",
    "# Function to scrape titles, descriptions, and paragraph indexes\n",
    "def scrape_titles_descriptions_and_indexes(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all paragraphs\n",
    "        paragraphs = soup.find_all('p')\n",
    "\n",
    "        # Find all titles (text within <strong> tags) and descriptions\n",
    "        data = []\n",
    "        for idx, p in enumerate(paragraphs):\n",
    "            strong_tags = p.find_all('strong')\n",
    "            if strong_tags:\n",
    "                title = strong_tags[0].get_text()\n",
    "                description = p.get_text().replace(title, '').strip()\n",
    "                data.append({'title': title, 'description': description, 'Paragraph Index': idx, 'category': 'romantic weekend'})\n",
    "\n",
    "        return data\n",
    "    else:\n",
    "        # If the request was not successful, print an error message\n",
    "        print(\"Failed to fetch the URL:\", url)\n",
    "        return None\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = 'https://www.choosechicago.com/articles/itineraries/48-hour-romance/'\n",
    "\n",
    "# Scrape image URLs and paragraph counts\n",
    "img_urls, p_counts = scrape_images_and_paragraphs(url)\n",
    "\n",
    "# Scrape titles, descriptions, and paragraph indexes\n",
    "title_description_data = scrape_titles_descriptions_and_indexes(url)\n",
    "\n",
    "# Create DataFrames from scraped data\n",
    "df_images = pd.DataFrame({'image_url': img_urls, 'Paragraphs Before Image': p_counts, 'category': 'image'})\n",
    "df_titles = pd.DataFrame(title_description_data)\n",
    "\n",
    "# Merge the DataFrames based on paragraph index\n",
    "merged_df = pd.merge(df_titles, df_images, left_on='Paragraph Index', right_on='Paragraphs Before Image', how='left')\n",
    "\n",
    "# Drop unnecessary columns (category_y)\n",
    "merged_df.drop(columns=['Paragraph Index', 'Paragraphs Before Image', 'category_y'], inplace=True)\n",
    "\n",
    "# Save the combined data to a CSV file\n",
    "merged_df.to_csv('stage1_data_CoupleRomanticWeekend.csv', index=False)\n",
    "\n",
    "print(\"Combined CSV file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c987c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Romantic Weekend cont.\n",
    "\n",
    "\n",
    "# Read the CSV file into a DataFrame, skipping the first row\n",
    "df = pd.read_csv('stage1_data_CoupleRomanticWeekend.csv', skiprows=[1])\n",
    "# print(df.columns)\n",
    "\n",
    "punctuation = '!‚Äù$%&\\‚Äô()*+,-./:;<=>?[\\\\]^_`{|}~‚Ä¢@¬©'\n",
    "\n",
    "def remove_links(text):\n",
    "    \"\"\"Takes a string and removes web links from it\"\"\"\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove http links\n",
    "    text = re.sub(r'bit.ly/\\S+', '', text)  # remove bitly links\n",
    "    text = text.strip('[link]')  # remove [links]\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'pic.twitter\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_tags(text):\n",
    "    remove = re.compile(r'')\n",
    "    return re.sub(remove, '', text)\n",
    "\n",
    "\n",
    "def remove_users(text):\n",
    "    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n",
    "    text = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove re-tweet\n",
    "    text = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove tweeted at\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    \"\"\"Takes a string and removes any hash tags\"\"\"\n",
    "    text = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove hash tags\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_av(text):\n",
    "    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n",
    "    text = re.sub('VIDEO:', '', text)  # remove 'VIDEO:' from start of tweet\n",
    "    text = re.sub('AUDIO:', '', text)  # remove 'AUDIO:' from start of tweet\n",
    "    return text\n",
    "\n",
    "\n",
    "def basic_clean(text):\n",
    "    \"\"\"Main master function to clean tweets only without tokenization or removal of stopwords\"\"\"\n",
    "    text = remove_users(text)\n",
    "    text = remove_links(text)\n",
    "    text = remove_hashtags(text)\n",
    "    text = remove_tags(text)\n",
    "    text = remove_av(text)\n",
    "    text = text.lower()  # lower case\n",
    "    text = re.sub('[' + punctuation + ']+', ' ', text)  # strip punctuation\n",
    "    text = re.sub('\\s+', ' ', text)  # remove double spacing\n",
    "    text = re.sub('([0-9]+)', '', text)  # remove numbers\n",
    "    text = re.sub('üìù ‚Ä¶', '', text)\n",
    "    text = re.sub('‚Äì', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    return [x for x in words if x not in stop_words]\n",
    "\n",
    "\n",
    "def lemmatize_word(text):\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    return \" \".join([wordnet.lemmatize(word) for word in text])\n",
    "\n",
    "# Apply basic cleaning to the 'description' column and store the result in 'text_cleaned'\n",
    "df['text_cleaned'] = df['description'].apply(basic_clean)\n",
    "# df['text_cleaned'] = df['text_cleaned'].apply(remove_stopwords)\n",
    "# df['text_cleaned'] = df['text_cleaned'].apply(lemmatize_word)\n",
    "\n",
    "df.to_csv('stage1_data_CoupleRomanticWeekend.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7a7a15bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 13 CSV files into '/Users/marianxu/Documents/ADSP34002/Couple_combined_file.csv'\n"
     ]
    }
   ],
   "source": [
    "## Combine all csv files\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the directory path where your CSV files are located\n",
    "directory_path = '/Users/marianxu/Documents/ADSP34002'\n",
    "\n",
    "# Get a list of all CSV files in the directory\n",
    "csv_files = [file for file in os.listdir(directory_path) if file.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes_list = []\n",
    "\n",
    "# Loop through each CSV file and read its contents into a DataFrame\n",
    "for file_name in csv_files:\n",
    "    file_path = os.path.join(directory_path, file_name)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dataframes_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "combined_df = pd.concat(dataframes_list, ignore_index=True)\n",
    "\n",
    "# Write the combined DataFrame to a new CSV file\n",
    "combined_file_path = '/Users/marianxu/Documents/ADSP34002/Couple_combined_file.csv'\n",
    "combined_df.to_csv(combined_file_path, index=False)\n",
    "\n",
    "print(f\"Combined {len(csv_files)} CSV files into '{combined_file_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77df9e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
